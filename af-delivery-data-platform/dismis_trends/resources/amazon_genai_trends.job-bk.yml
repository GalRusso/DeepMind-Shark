# GenAI Trends Data Pipeline Job
# This job orchestrates the end-to-end data flow:
# 1. Ingestion: Extract data from Monday.com and save to S3
# 2. Transformation: Process data through DLT medallion architecture

resources:
  jobs:
    amazon_genai_trends_pipeline_job:
      name: "Amazon GenAI Trends Job"
      description: "End-to-end pipeline for Amazon GenAI trends data from Monday.com to Unity Catalog"
      tags:
        env: ${bundle.target}
        vertical: dismis

      parameters:
        - name: environment
          default: ${bundle.target}
        - name: board_id
          default: "8505954252"
        - name: group_id
          default: "group_title"
        - name: vertical
          default: dismis
        - name: data_domain
          default: trends
        - name: file_name_prefix
          default: amazon_genai_trends

      # Schedule configuration - adjust as needed
      schedule:
        quartz_cron_expression: "0 0 0 * * ?"
        timezone_id: "Asia/Ho_Chi_Minh"

      # Notification settings
      # email_notifications:
      #   on_failure:
      #     - hoangl@activefence.com  # Update with your team email

      # Job configuration
      max_concurrent_runs: 1
      timeout_seconds: 7200 # 2 hours timeout

      # Task definitions
      tasks:
        # Task 1: Ingest data from Monday.com to S3
        - task_key: ingest_monday_to_s3
          notebook_task:
            notebook_path: ../src/data_ingestion/monday2s3.ipynb
          timeout_seconds: 1800 # 30 minutes timeout
          max_retries: 2
          min_retry_interval_millis: 60000 # 1 minute

        # Task 2: Run DLT Pipeline (S3 -> UC (Bronze -> Silver -> Gold))
        - task_key: run_dlt_pipeline
          depends_on:
            - task_key: ingest_monday_to_s3
          pipeline_task:
            pipeline_id: ${resources.pipelines.amazon_genai_trends_pipeline.id}
          timeout_seconds: 3600 # 1 hour timeout
