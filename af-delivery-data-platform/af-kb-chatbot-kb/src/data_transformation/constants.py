from databricks.sdk.runtime import dbutils, spark
from pyspark.sql.types import (
    ArrayType,
    BinaryType,
    DoubleType,
    LongType,
    MapType,
    StringType,
    StructField,
    StructType,
    TimestampType,
)

dbutils.widgets.text("ENVIRONMENT", "dev")
ENVIRONMENT = dbutils.widgets.get("ENVIRONMENT")

VERTICAL = "kb_unstructured"
SOURCE = "monday"
CATALOG = f"af_delivery_{ENVIRONMENT}"
SOURCE_PATH = f"/Volumes/af_delivery_{ENVIRONMENT}/data_collection/{VERTICAL}/all"
MONDAY_SOURCE_PATH = f"/Volumes/af_delivery_{ENVIRONMENT}/data_collection/{VERTICAL}/finalized_documents"


BRONZE_NAMESPACE = f"{CATALOG}.landing_{VERTICAL}"
SILVER_NAMESPACE = f"{CATALOG}.silver_{VERTICAL}"
GOLD_NAMESPACE = f"{CATALOG}.gold_{VERTICAL}"

LANDING_TABLE_NAME = f"{BRONZE_NAMESPACE}.{SOURCE}"
SILVER_CHUNKING_TABLE = f"{SILVER_NAMESPACE}.chunking"
SILVER_CHUNKING_DLQ_TABLE = f"{SILVER_NAMESPACE}.chunking_dlq"
SILVER_VECTORIZING_TABLE = f"{SILVER_NAMESPACE}.vectorizing"
GOLD_TABLE = f"{GOLD_NAMESPACE}.gold"
GOLD_TRACKING_TABLE = f"{GOLD_NAMESPACE}.gold_tracking"

BRONZE_SCHEMA = StructType(
    [
        StructField("landing_id", StringType(), False, metadata={"comment": "Unique UUID of this landing"}),
        StructField("file_name", StringType(), True, metadata={"comment": "Name of the ingested file"}),
        StructField("file_hash", StringType(), False, metadata={"comment": "SHA-256 hash of content (required)"}),
        StructField("file_size", LongType(), False, metadata={"comment": "Size in bytes"}),
        StructField("mime_type", StringType(), False, metadata={"comment": "Inferred mimetype (required)"}),
        StructField("ingestion_ts", TimestampType(), True, metadata={"comment": "Timestamp of data ingestion"}),
        StructField("source_file", StringType(), True, metadata={"comment": "Source file path"}),
        StructField("metadata_file", StringType(), True, metadata={"comment": "Metadata file path"}),
        StructField("original_url", StringType(), True, metadata={"comment": "Google View URL of the file"}),
        StructField("gdrive_path", StringType(), True, metadata={"comment": "Google Drive path of the file"}),
        StructField("modified_time", TimestampType(), True, metadata={"comment": "modifiedTime in google drive"}),
        StructField("team", StringType(), True, metadata={"comment": "Team from Monday board"}),
        StructField("client", StringType(), True, metadata={"comment": "Client from Monday board"}),
        StructField("type", StringType(), True, metadata={"comment": "Document type from Monday board"}),
        StructField("delivery_date", StringType(), True, metadata={"comment": "Delivery date from Monday board"}),
        StructField("owner", ArrayType(StringType()), True, metadata={"comment": "Owner(s) from Monday board"}),
        StructField("bug_number", StringType(), True, metadata={"comment": "Bug number from Monday board"}),
        StructField("report_name", StringType(), True, metadata={"comment": "Report name from Monday board"}),
        StructField(
            "board_id",
            StringType(),
            True,
            metadata={"comment": "monday board_id, will be use for updating the board after processed"},
        ),
        StructField(
            "group_id",
            StringType(),
            True,
            metadata={"comment": "monday group_id, will be use for updating the board after processed"},
        ),
        StructField(
            "item_id",
            StringType(),
            True,
            metadata={"comment": "monday item_id, will be use for updating the board after processed"},
        ),
        StructField("landing_ts", TimestampType(), True, metadata={"comment": "Timestamp of data landing"}),
    ]
)

SILVER_CHUNKING_SCHEMA = StructType(
    [
        StructField("landing_id", StringType(), False, metadata={"comment": "Unique UUID of the landing"}),
        StructField("chunk_id", StringType(), False),
        StructField("text", StringType(), True),
        StructField(
            "metadata",
            MapType(StringType(), StringType()),
            True,
            metadata={"comment": "Metadata of the chunk, generated by docling"},
        ),
        StructField("created_at", TimestampType(), True),
    ]
)

SILVER_CHUNKING_DLQ_SCHEMA = StructType(
    [
        StructField("landing_id", StringType(), False, metadata={"comment": "Unique UUID of the landing"}),
        StructField("source_file", StringType(), True, metadata={"comment": "Source file path for debugging"}),
        StructField("error_message", StringType(), True, metadata={"comment": "Error message from parsing failure"}),
        StructField("failed_at", TimestampType(), True, metadata={"comment": "Timestamp when parsing failed"}),
    ]
)

SILVER_VECTORIZING_SCHEMA = StructType(
    [
        StructField("landing_id", StringType(), False, metadata={"comment": "Unique UUID of the landing"}),
        StructField("chunk_id", StringType(), False),
        StructField("embedding", ArrayType(DoubleType()), True),
        StructField("embedding_binary", BinaryType(), True),
        StructField("embed_model", StringType(), True),
        StructField("created_at", TimestampType(), True),
    ]
)

spark.createDataFrame([], SILVER_CHUNKING_SCHEMA).write.mode("ignore").format("delta").saveAsTable(
    SILVER_CHUNKING_TABLE
)

spark.createDataFrame([], SILVER_CHUNKING_DLQ_SCHEMA).write.mode("ignore").format("delta").saveAsTable(
    SILVER_CHUNKING_DLQ_TABLE
)

spark.createDataFrame([], SILVER_VECTORIZING_SCHEMA).write.mode("ignore").format("delta").saveAsTable(
    SILVER_VECTORIZING_TABLE
)

GOLD_SCHEMA = StructType(
    [
        StructField("landing_id", StringType(), False, metadata={"comment": "Unique UUID of the landing"}),
        StructField("chunk_id", StringType(), False),
        StructField("original_url", StringType(), True, metadata={"comment": "Original URL of the file"}),
        StructField("vertical", StringType(), False, metadata={"comment": "Vertical of the data"}),
        StructField("text", StringType(), False, metadata={"comment": "Text of the chunk"}),
        StructField(
            "metadata",
            StructType(
                [
                    StructField("file_path", StringType(), True),
                    StructField("vertical", StringType(), True),
                    StructField("last_modified", LongType(), True),
                    StructField("client", StringType(), True),
                    StructField("delivery_date", LongType(), True),
                    StructField("subtype", StringType(), True),
                    StructField("owner", ArrayType(StringType()), True),
                    StructField("bug_number", StringType(), True),
                    StructField("report_name", StringType(), True),
                ]
            ),
            True,
            metadata={"comment": "Queriable metadata, for inserting to pinecone"},
        ),
        StructField("embedding", ArrayType(DoubleType()), False, metadata={"comment": "Embedding of the chunk"}),
        StructField("embedding_binary", BinaryType(), True, metadata={"comment": "Binary embedding of the chunk"}),
        StructField("created_at", TimestampType(), False, metadata={"comment": "Timestamp of creation"}),
        StructField(
            "board_id",
            StringType(),
            True,
            metadata={"comment": "Monday board_id for closed-loop status updates"},
        ),
        StructField(
            "group_id",
            StringType(),
            True,
            metadata={"comment": "Monday group_id for closed-loop status updates"},
        ),
        StructField(
            "item_id",
            StringType(),
            True,
            metadata={"comment": "Monday item_id for closed-loop status updates"},
        ),
    ]
)

GOLD_TRACKING_SCHEMA = StructType(
    [
        StructField(
            "last_processed_timestamp", TimestampType(), False, metadata={"comment": "Timestamp of last processed data"}
        ),
        StructField("pinecone_stats", StringType(), True, metadata={"comment": "Stats of pinecone, it's fun to watch"}),
    ]
)

spark.createDataFrame([], GOLD_TRACKING_SCHEMA).write.mode("ignore").format("delta").saveAsTable(GOLD_TRACKING_TABLE)
